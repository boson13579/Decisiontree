#%%
import sklearn.metrics
from sklearn import datasets
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import math
import numpy as np
import pandas as pd
import random

stoi = {'low': 1, 'med': 2, 'high': 3, 'vhigh': 4, '2': 1, '3': 2, '4': 3, '5more':4, 'more':4, 'big':3, 'small': 1}


class Node:
    "Decision tree node"
    def __init__(self, entropy, num_samples, num_samples_per_class, predicted_class, num_errors, alpha=float("inf")):
        self.entropy = entropy # the entropy of current node
        self.num_samples = num_samples
        self.num_samples_per_class = num_samples_per_class
        self.predicted_class = predicted_class # the majority class of the split group
        self.feature_index = 0 # the feature index we used to split the node
        self.threshold = 0 # for binary split
        self.left = None # left child node
        self.right = None # right child node
        self.num_errors = num_errors # error after cut
        self.alpha = alpha # each node alpha

#%%
class DecisionTreeClassifier:
    def __init__(self, max_depth=4):
        self.max_depth = max_depth
        self.n_classes_ = None  # a property to let you store the number of classes

    def _entropy(self,sample_y,n_classes):
        # TODO: calculate the entropy of sample_y and return it
        # sample_y represent the label of node
        # entropy = -sum(pi * log2(pi))
        entropy = 0

        # TODO: check range of n_classes
        if sample_y.size == 0:
            return 0
        
        for i in range(n_classes):
            pi = np.sum(sample_y == i) / sample_y.size
            if pi != 0:
                entropy -= pi * math.log2(pi)

        return entropy

    def _feature_split(self, X, y,n_classes):
        # Returns:
        #  best_idx: Index of the feature for best split, or None if no split is found.
        #  best_thr: Threshold to use for the split, or None if no split is found.
        mx = len(np.unique(y))
        # print("unique", mx)
        if mx <= 1:
            return None, None
        
        # Entropy of current node.

        best_criterion = self._entropy(y,n_classes)

        best_idx, best_thr = None, None
        # TODO: find the best split, loop through all the features, and consider all the
        # midpoints between adjacent training samples as possible thresholds. 
        # Compute the Entropy impurity of the split generated by that particular feature/threshold
        # pair, and return the pair with smallest impurity.

        for i in range(X.shape[1]):
            splitnum = np.unique(X[:,i])
            for t in splitnum:
                left_y = y[X[:,i] <= t]
                right_y = y[X[:,i] > t]
                left_criterion = self._entropy(left_y,n_classes)
                right_criterion = self._entropy(right_y,n_classes)
                
                criterion = left_criterion*(left_y.size/y.size) + right_criterion*(right_y.size/y.size)
                if criterion < best_criterion:
                    best_criterion = criterion
                    best_idx = i
                    best_thr = t
                    
        return best_idx, best_thr
    def _build_tree(self, X, y, depth=0):
        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]
        predicted_class = np.argmax(num_samples_per_class)
        correct_label_num = num_samples_per_class[predicted_class]
        num_errors = y.size - correct_label_num
        node = Node(
            entropy = self._entropy(y,self.n_classes_),
            num_samples=y.size,
            num_samples_per_class=num_samples_per_class,
            predicted_class=predicted_class,
            num_errors=num_errors
        )

        if depth < self.max_depth:
            idx, thr = self._feature_split(X, y,self.n_classes_)
            # TODO: Split the tree recursively according index and threshold until maximum depth is reached.
            
            if idx is not None:
                node.left = self._build_tree(X[X[:,idx] <= thr], y[X[:,idx] <= thr], depth + 1)
                node.right = self._build_tree(X[X[:,idx] > thr], y[X[:,idx] > thr], depth + 1)
                node.feature_index = idx
                node.threshold = thr
            
                pass
        return node

    def fit(self,X,Y):
        # TODO
        # Fits to the given training data
        self.n_classes_ = len(np.unique(Y))
        self.root = self._build_tree(X, Y, 0)

    def predict(self,X):
        pred = []
        #TODO: predict the label of data
        for i in range(X.shape[0]):
            now = self.root
            while(now.left != None):
                if X[i,now.feature_index] <= now.threshold:
                    now = now.left
                else:
                    now = now.right    
            pred.append(now.predicted_class)

        return pred

    def _find_leaves(self, root):
        #TODO
        ## find each node child leaves number
        ret = []
        if(root.left == None and root.right == None):
            return [root]
        else:
            if root.left != None:
                ret += self._find_leaves(root.left)
            if root.right != None:
                ret += self._find_leaves(root.right)
        return ret

    def _error_before_cut(self, root):
        # TODO
        ## return error before post-pruning
        leaves = self._find_leaves(root)
        sum = 0
        for i in leaves:
            sum += i.num_errors   
        return sum

    def _compute_alpha(self, root):
        # TODO
        ## Compute each node alpha
        # alpha = (error after cut - error before cut) / (leaves been cut - 1)
        return (root.num_errors - self._error_before_cut(root))/(len(self._find_leaves(root)) - 1)
    
    def _find_min_alpha(self, root):
        MinAlpha = float("inf")
        # TODO
        ## Search the Decision tree which have minimum alpha's node
        ret = [None, float("inf")]
        if root.left == None and root.right == None:
            return [root, float("inf")]

        rootAlpha = self._compute_alpha(root)
        if ret[1] > rootAlpha:
            ret[0] = root
            ret[1] = rootAlpha
        if root.left != None:
            tmp = self._find_min_alpha(root.left)
            if ret[1] > tmp[1]:
                ret[0] = tmp[0]
                ret[1] = tmp[1]
        if root.right != None:
            tmp = self._find_min_alpha(root.right)
            if ret[1] > tmp[1]:
                ret[0] = tmp[0]
                ret[1] = tmp[1]
        return ret
            

    def _prune(self):
        # TODO
        # prune the decision tree with minimum alpha node
        now = self._find_min_alpha(self.root)[0]
        def dfs(root):
            if root == now:
                root.left = None
                root.right = None
                return
            if root.left != None:
                dfs(root.left)
            if root.right != None:
                dfs(root.right)
        dfs(self.root)
    
    def ptree(self):
        def dfs(root):
            if(root == None): return 0 
            ret = 1
            ret += dfs(root.left)
            ret += dfs(root.right)
            return ret
        return dfs(self.root)
        
#%%
def load_train_test_data(test_ratio=.3, random_state = 1):
    df = pd.read_csv('./car.data', names=['buying', 'maint',
                     'doors', 'persons', 'lug_boot', 'safety', 'target'])
    X = df.drop(columns=['target'])
    X = np.array(X.values)
    y = np.array(df['target'].values)
    label = np.unique(y)
    # label encoding
    for i in range(len(y)):
        for j in range(len(label)):
            if y[i] == label[j]:
                y[i] = j
                break
    y = y.astype('int')

    #maybe it will be ligel
    for i in range(X.shape[0]):
        for t in range(X.shape[1]):
            X[i,t] = stoi[X[i,t]]
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size = test_ratio, random_state=random_state, stratify=y)
    return X_train, X_test, y_train, y_test


def accuracy_report(X_train_scale, y_train,X_test_scale,y_test,max_depth=7):
    tree = DecisionTreeClassifier( max_depth=max_depth)
    tree.fit(X_train_scale, y_train)
    print("tree size: %d" % tree.ptree())    

    pred = tree.predict(X_train_scale)
    print("tree train accuracy: %f" 
        % (sklearn.metrics.accuracy_score(y_train, pred )))
    pred = tree.predict(X_test_scale)
    print("tree test accuracy: %f" 
        % (sklearn.metrics.accuracy_score(y_test, pred )))

    for i in range(10):
        print("=============Cut %d=============" % (i+1))
        tree._prune()
        print("tree size: %d" % tree.ptree())
        pred = tree.predict(X_train_scale)
        print("tree train accuracy: %f"
              % (sklearn.metrics.accuracy_score(y_train, pred)))
        pred = tree.predict(X_test_scale)
        print("tree test accuracy: %f"
              % (sklearn.metrics.accuracy_score(y_test, pred)))
#%%    
def main():
    random_state = 1
    # random_state = random.randint(0,4294967295)
    # print("random state: %d" % random_state)
    X_train, X_test, y_train, y_test = load_train_test_data(test_ratio=.3,random_state = random_state)  
    accuracy_report(X_train, y_train,X_test,y_test,max_depth=8)
#%%
if __name__ == "__main__":
    main()

# #%%

# from matplotlib import pyplot as plt

# train_ac = []
# test_ac = []
# def accuracy_report(X_train_scale, y_train,X_test_scale,y_test,max_depth=7):
#     tree = DecisionTreeClassifier( max_depth=max_depth)
#     tree.fit(X_train_scale, y_train)
#     # print("tree size: %d" % tree.ptree())    

#     pred = tree.predict(X_train_scale)
#     # print("tree train accuracy: %f" 
#     #     % (sklearn.metrics.accuracy_score(y_train, pred )))
#     pred = tree.predict(X_test_scale)
#     # print("tree test accuracy: %f" 
#     #     % (sklearn.metrics.accuracy_score(y_test, pred )))

#     for i in range(30):
#         # print("=============Cut %d=============" % (i+1))
#         tree._prune()
#         # print("tree size: %d" % tree.ptree())
#         pred = tree.predict(X_train_scale)
#         # print("tree train accuracy: %f"
#         #       % (sklearn.metrics.accuracy_score(y_train, pred)))
#         train_ac.append(sklearn.metrics.accuracy_score(y_train, pred))
#         pred = tree.predict(X_test_scale)
#         # print("tree test accuracy: %f"
#         #      % (sklearn.metrics.accuracy_score(y_test, pred)))
#         test_ac.append(sklearn.metrics.accuracy_score(y_test, pred))

#     #plot test_ac and train_ac
#     plt.plot(range(1, 31), train_ac, label='Train Accuracy')
#     plt.plot(range(1, 31), test_ac, label='Test Accuracy')
#     plt.xlabel('Number of Cuts')
#     plt.ylabel('Accuracy')
#     plt.title('Accuracy vs Number of Cuts')
#     plt.legend()
#     plt.show()
#     test_ac.clear()
#     train_ac.clear()

